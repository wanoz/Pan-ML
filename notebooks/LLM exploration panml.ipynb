{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d888c13",
   "metadata": {},
   "source": [
    "## Assess various language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54128d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "from transformers import TrainingArguments, Trainer, Seq2SeqTrainer, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfe1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device type\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9d831",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ad7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading question and answer data...\n",
      "Place data into dataframe object...\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "data_dir = os.path.join(base_dir, 'data', 'text_question_answer')\n",
    "\n",
    "print('Loading question and answer data...')\n",
    "with open (os.path.join(data_dir, 'train-squad-v2.0.json'), 'r') as f:\n",
    "    train_qna_raw = json.loads(f.read())\n",
    "\n",
    "\n",
    "print('Place data into dataframe object...')\n",
    "# Processing SQUAD data\n",
    "df_train = None\n",
    "ids = []\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "is_impossible = []\n",
    "for il1, l1 in enumerate(train_qna_raw['data']):\n",
    "    for il2, l2 in enumerate(l1['paragraphs']):\n",
    "        for il3, l3 in enumerate(l2['qas']):\n",
    "            ids.append(l3['id'])\n",
    "            contexts.append(l2['context'])\n",
    "            questions.append(l3['question'])\n",
    "            try:\n",
    "                answers.append(l3['answers'][0]['text'])\n",
    "            except:\n",
    "                answers.append('')\n",
    "            is_impossible.append(l3['is_impossible'])\n",
    "            \n",
    "df_train = pd.DataFrame({'id': ids, 'context': contexts, \n",
    "                         'question': questions, 'answer': answers, 'is_impossible': is_impossible})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af508e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "1  56be85543aeaaa14008c9065   \n",
       "2  56be85543aeaaa14008c9066   \n",
       "3  56bf6b0f3aeaaa14008c9601   \n",
       "4  56bf6b0f3aeaaa14008c9602   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "   is_impossible  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f989f3",
   "metadata": {},
   "source": [
    "### Text generation (casual language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f296f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateHuggingFaceLM():\n",
    "    '''\n",
    "    Generic language model class using the HuggingFace Hub models\n",
    "    '''\n",
    "    # Initialize class variables\n",
    "    def __init__(self, model, tokenizer, input_block_size, padding_length=100):\n",
    "        self.padding_length = padding_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.input_block_size = input_block_size\n",
    "        self.train_default_args = ['title', 'num_train_epochs', 'optimizer', 'mlm', \n",
    "                                   'per_device_train_batch_size', 'per_device_eval_batch_size',\n",
    "                                   'warmup_steps', 'weight_decay', 'logging_steps', \n",
    "                                   'output_dir', 'logging_dir', 'save_model']\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.model.config.eos_token_id\n",
    "    \n",
    "    # Embed text\n",
    "    def embedding(self, text):\n",
    "        token_ids = self.tokenizer.encode(text, return_tensors='pt')\n",
    "        \n",
    "        # Get embeddings\n",
    "        if 'flan' in self.model.name_or_path: \n",
    "            emb = self.model.shared.weight[token_ids[0]] # embeddings for FLAN\n",
    "        elif 'gpt2' in self.model.name_or_path: \n",
    "            emb = self.model.transformer.wte.weight[token_ids[0]] # embeddings for GPT2\n",
    "            \n",
    "        emb /= emb.norm(dim=1).unsqueeze(1) # normalise embedding weights\n",
    "        emb_pad = torch.zeros(self.padding_length, emb.shape[1]) # Set and apply padding to embeddings\n",
    "        emb_pad[:emb.shape[0], :] = emb\n",
    "        \n",
    "        return emb_pad\n",
    "    \n",
    "    # Generate text\n",
    "    def predict(self, text, max_length=100, skip_special_tokens=True, show_scores=False):\n",
    "        output_context = {\n",
    "            'text': None,\n",
    "            'scores': None,\n",
    "        }\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(text, return_tensors='pt')\n",
    "        output = self.model.generate(input_ids, max_length=max_length, \n",
    "                                     pad_token_id=self.model.config.eos_token_id,\n",
    "                                     num_return_sequences=1, output_scores=show_scores, \n",
    "                                     return_dict_in_generate=show_scores,\n",
    "                                     renormalize_logits=show_scores)\n",
    "                                          \n",
    "        # Decode the output sequence\n",
    "        if show_scores:\n",
    "            output_context['text'] = self.tokenizer.decode(output['sequences'][0], skip_special_tokens=skip_special_tokens)\n",
    "            output_context['scores'] = output['scores']\n",
    "        else:\n",
    "            output_context['text'] = self.tokenizer.decode(output[0], skip_special_tokens=skip_special_tokens)\n",
    "        output_context['text'] = output_context['text'].replace('\\n', '')\n",
    "        output_context['text'] = output_context['text'].strip()\n",
    "        \n",
    "        return output_context\n",
    "    \n",
    "    # Tokenize function\n",
    "    def _tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples['text'])\n",
    "    \n",
    "    # Tokenize pandas dataframe feature\n",
    "    def tokenize_text(self, x, batched=False):  \n",
    "        df_sample = pd.DataFrame({'text': x})\n",
    "        hf_dataset = Dataset.from_pandas(df_sample)\n",
    "        if batched:\n",
    "            tokenized_dataset = hf_dataset.map(self._tokenize_function, batched=batched, num_proc=4)\n",
    "        else:\n",
    "            tokenized_dataset = hf_dataset.map(self._tokenize_function)\n",
    "\n",
    "        return tokenized_dataset\n",
    "    \n",
    "    # Model training\n",
    "    def fit(self, x, y, train_args={}, instruct=False):\n",
    "        # Convert to tokens format from pandas dataframes\n",
    "        tokenized_data = self.tokenize_text(x)\n",
    "        tokenized_target = self.tokenize_text(y)\n",
    "        \n",
    "        # Check for missing input arguments\n",
    "        assert set(list(train_args.keys())) == set(self.train_default_args), \\\n",
    "        f'Train args are not in the required format - missing: {\", \".join(list(set(self.train_default_args) - set(list(train_args.keys()))))}'\n",
    "        \n",
    "        if instruct:\n",
    "            print('Setting up training in sequence to sequence format...')\n",
    "            tokenized_data = tokenized_data.add_column('labels', tokenized_target['input_ids']) # Create target sequence labels\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer=self.tokenizer, model=self.model) # Organise data for training\n",
    "            \n",
    "            # Setup training in sequence to sequence format\n",
    "            training_args = TrainingArguments(\n",
    "                optim=train_args['optimizer'], # model optimisation function\n",
    "                num_train_epochs=train_args['num_train_epochs'], # total number of training epochs\n",
    "                per_device_train_batch_size=train_args['per_device_train_batch_size'],  # batch size per device during training\n",
    "                per_device_eval_batch_size=train_args['per_device_eval_batch_size'],   # batch size for evaluation\n",
    "                warmup_steps=train_args['warmup_steps'], # number of warmup steps for learning rate scheduler\n",
    "                weight_decay=train_args['weight_decay'], # strength of weight decay\n",
    "                logging_steps=train_args['logging_steps'],\n",
    "                output_dir='./results', # output directory\n",
    "                logging_dir=train_args['logging_dir'], # log directory\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_data.remove_columns(['text']),\n",
    "                eval_dataset=tokenized_data.remove_columns(['text']),\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "    \n",
    "        else:\n",
    "            print('Setting up training in autoregressive format...')\n",
    "            data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=train_args['mlm']) # Organise data for training\n",
    "            \n",
    "            # Setup training in autoregressive format\n",
    "            training_args = TrainingArguments(\n",
    "                optim=train_args['optimizer'], \n",
    "                num_train_epochs=train_args['num_train_epochs'], \n",
    "                per_device_train_batch_size=train_args['per_device_train_batch_size'],\n",
    "                per_device_eval_batch_size=train_args['per_device_eval_batch_size'], \n",
    "                warmup_steps=train_args['warmup_steps'], \n",
    "                weight_decay=train_args['weight_decay'], \n",
    "                logging_steps=train_args['logging_steps'],\n",
    "                output_dir=train_args['output_dir'],\n",
    "                logging_dir=train_args['logging_dir'], \n",
    "            )\n",
    "            \n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_data.remove_columns(['text']),\n",
    "                eval_dataset=tokenized_data.remove_columns(['text']),\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "\n",
    "        trainer.train() # Execute training\n",
    "        \n",
    "        if train_args['save_model']:\n",
    "            trainer.save_model(f'./results/model_{train_args[\"title\"]}') # Save trained model\n",
    "                                          \n",
    "# Calculate cosine similarity of two matrices    \n",
    "def cosine_similarity(m1, m2):\n",
    "    return F.cosine_similarity(m1.view(1, -1), m2.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac382071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# GPT2 small\n",
    "tokenizer_dgpt2 = AutoTokenizer.from_pretrained('distilgpt2', mirror='https://huggingface.co')\n",
    "model_dgpt2 = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Create HuggingFace LM from base HuggingFace model and tokenizer objects\n",
    "model_dgpt2_train=copy.deepcopy(model_dgpt2)\n",
    "lm_gpt2 = CreateHuggingFaceLM(model_dgpt2, tokenizer_dgpt2, input_block_size=10, padding_length=100)\n",
    "lm_gpt2_train = CreateHuggingFaceLM(model_dgpt2_train, tokenizer_dgpt2, input_block_size=10, padding_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b6f7eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -6.1587,  -5.8585,  -8.0755,  ..., -15.7681, -12.8356,  -7.1570]]),\n",
       " tensor([[-10.1662,  -9.6845,  -9.8448,  ..., -17.6942, -15.0358,  -5.2994]]),\n",
       " tensor([[-14.8451,  -5.4011, -12.4950,  ..., -33.7572, -34.8521, -10.4029]]),\n",
       " tensor([[-15.3718,  -5.4762, -12.5057,  ..., -33.1928, -34.5009,  -9.8682]]),\n",
       " tensor([[-14.6399,  -4.8943, -11.0783,  ..., -30.5098, -31.7271,  -8.5576]]),\n",
       " tensor([[-14.6345,  -5.1920, -10.9806,  ..., -29.2536, -30.4836,  -7.8928]]),\n",
       " tensor([[-14.9126,  -5.9431, -11.2593,  ..., -28.4916, -29.7005,  -7.4626]]),\n",
       " tensor([[-15.0764,  -6.5687, -11.3499,  ..., -27.8648, -28.9608,  -7.1299]]),\n",
       " tensor([[-15.1187,  -6.9986, -11.2854,  ..., -27.3189, -28.2459,  -6.8514]]),\n",
       " tensor([[-15.0900,  -7.2397, -11.1368,  ..., -26.9202, -27.6835,  -6.6504]]),\n",
       " tensor([[-15.0409,  -7.4004, -10.9969,  ..., -26.6229, -27.2407,  -6.4946]]),\n",
       " tensor([[-14.9937,  -7.4935, -10.8786,  ..., -26.4740, -26.9780,  -6.4035]]),\n",
       " tensor([[-14.9631,  -7.5899, -10.7964,  ..., -26.3625, -26.7677,  -6.3340]]),\n",
       " tensor([[-14.9220,  -7.6311, -10.7204,  ..., -26.3153, -26.6402,  -6.2910]]),\n",
       " tensor([[-14.8963,  -7.6820, -10.6766,  ..., -26.3251, -26.5816,  -6.2682]]),\n",
       " tensor([[-14.8730,  -7.7252, -10.6395,  ..., -26.3374, -26.5355,  -6.2562]]),\n",
       " tensor([[-14.8615,  -7.7832, -10.6326,  ..., -26.3624, -26.5048,  -6.2450]]),\n",
       " tensor([[-14.8473,  -7.8070, -10.6209,  ..., -26.4320, -26.5323,  -6.2611]]),\n",
       " tensor([[-14.8433,  -7.8612, -10.6364,  ..., -26.4757, -26.5363,  -6.2632]]),\n",
       " tensor([[-14.8415,  -7.8905, -10.6418,  ..., -26.5478, -26.5759,  -6.2835]]),\n",
       " tensor([[-14.8426,  -7.9379, -10.6664,  ..., -26.6185, -26.6090,  -6.2992]]),\n",
       " tensor([[-14.8478,  -7.9724, -10.6919,  ..., -26.6955, -26.6607,  -6.3270]]),\n",
       " tensor([[-14.8437,  -7.9997, -10.7106,  ..., -26.7644, -26.6969,  -6.3457]]),\n",
       " tensor([[-14.8678,  -8.0467, -10.7572,  ..., -26.8698, -26.7802,  -6.3802]]),\n",
       " tensor([[-14.8826,  -8.0931, -10.7957,  ..., -26.9574, -26.8473,  -6.4080]]),\n",
       " tensor([[-14.8783,  -8.1104, -10.8187,  ..., -27.0351, -26.9018,  -6.4369]]),\n",
       " tensor([[-14.8967,  -8.1450, -10.8563,  ..., -27.1297, -26.9858,  -6.4698]]),\n",
       " tensor([[-14.9174,  -8.1952, -10.9146,  ..., -27.2242, -27.0624,  -6.5038]]),\n",
       " tensor([[-14.9330,  -8.2331, -10.9533,  ..., -27.3146, -27.1403,  -6.5375]]),\n",
       " tensor([[-14.9468,  -8.2562, -10.9951,  ..., -27.4258, -27.2440,  -6.5786]]),\n",
       " tensor([[-14.9598,  -8.2933, -11.0429,  ..., -27.5222, -27.3306,  -6.6071]]),\n",
       " tensor([[-14.9810,  -8.3357, -11.0957,  ..., -27.6244, -27.4344,  -6.6443]]),\n",
       " tensor([[-14.9867,  -8.3554, -11.1345,  ..., -27.7299, -27.5320,  -6.6844]]),\n",
       " tensor([[-15.0116,  -8.3947, -11.1904,  ..., -27.8510, -27.6617,  -6.7231]]),\n",
       " tensor([[-15.0335,  -8.4286, -11.2453,  ..., -27.9684, -27.7775,  -6.7594]]),\n",
       " tensor([[-15.0455,  -8.4565, -11.2873,  ..., -28.0814, -27.8978,  -6.7936]]),\n",
       " tensor([[-15.0640,  -8.4844, -11.3429,  ..., -28.2217, -28.0453,  -6.8426]]),\n",
       " tensor([[-15.0663,  -8.5026, -11.3805,  ..., -28.3167, -28.1445,  -6.8741]]),\n",
       " tensor([[-15.0915,  -8.5353, -11.4357,  ..., -28.4487, -28.2902,  -6.9141]]),\n",
       " tensor([[-15.1037,  -8.5536, -11.4826,  ..., -28.5861, -28.4333,  -6.9549]]),\n",
       " tensor([[-15.1230,  -8.5862, -11.5366,  ..., -28.7089, -28.5745,  -6.9909]]),\n",
       " tensor([[-15.1340,  -8.6069, -11.5836,  ..., -28.8451, -28.7160,  -7.0389]]),\n",
       " tensor([[-15.1469,  -8.6361, -11.6323,  ..., -28.9622, -28.8463,  -7.0696]]),\n",
       " tensor([[-15.1594,  -8.6587, -11.6742,  ..., -29.0716, -28.9769,  -7.1057]]),\n",
       " tensor([[-15.1761,  -8.6721, -11.7186,  ..., -29.2007, -29.1165,  -7.1494]]),\n",
       " tensor([[-15.1850,  -8.6896, -11.7559,  ..., -29.3069, -29.2324,  -7.1824]]),\n",
       " tensor([[-15.2031,  -8.7195, -11.8102,  ..., -29.4406, -29.3888,  -7.2215]]),\n",
       " tensor([[-15.2096,  -8.7359, -11.8479,  ..., -29.5372, -29.4976,  -7.2509]]),\n",
       " tensor([[-15.2242,  -8.7633, -11.8955,  ..., -29.6581, -29.6380,  -7.2865]]),\n",
       " tensor([[-15.2344,  -8.7791, -11.9315,  ..., -29.7681, -29.7602,  -7.3203]]),\n",
       " tensor([[-15.2435,  -8.7986, -11.9711,  ..., -29.8624, -29.8725,  -7.3478]]),\n",
       " tensor([[-15.2539,  -8.8108, -12.0041,  ..., -29.9636, -29.9837,  -7.3838]]),\n",
       " tensor([[-15.2671,  -8.8382, -12.0493,  ..., -30.0653, -30.1072,  -7.4109]]),\n",
       " tensor([[-15.2755,  -8.8485, -12.0726,  ..., -30.1415, -30.1974,  -7.4405]]),\n",
       " tensor([[-15.2945,  -8.8820, -12.1196,  ..., -30.2399, -30.3180,  -7.4590]]),\n",
       " tensor([[-15.2985,  -8.8962, -12.1483,  ..., -30.3394, -30.4339,  -7.4932]]),\n",
       " tensor([[-15.3108,  -8.9238, -12.1910,  ..., -30.4338, -30.5493,  -7.5168]]),\n",
       " tensor([[-15.3186,  -8.9329, -12.2155,  ..., -30.5169, -30.6430,  -7.5441]]),\n",
       " tensor([[-15.3254,  -8.9607, -12.2547,  ..., -30.6020, -30.7535,  -7.5625]]),\n",
       " tensor([[-15.3363,  -8.9822, -12.2869,  ..., -30.6788, -30.8496,  -7.5871]]),\n",
       " tensor([[-15.3365,  -8.9985, -12.3101,  ..., -30.7424, -30.9303,  -7.6069]]),\n",
       " tensor([[-15.3510,  -9.0164, -12.3431,  ..., -30.8315, -31.0385,  -7.6325]]),\n",
       " tensor([[-15.3483,  -9.0256, -12.3637,  ..., -30.8912, -31.1119,  -7.6497]]),\n",
       " tensor([[-15.3539,  -9.0354, -12.3838,  ..., -30.9653, -31.1974,  -7.6836]]),\n",
       " tensor([[-15.3542,  -9.0613, -12.4141,  ..., -31.0357, -31.2945,  -7.6907]]),\n",
       " tensor([[-15.3563,  -9.0772, -12.4328,  ..., -31.0858, -31.3622,  -7.7077]]),\n",
       " tensor([[-15.3533,  -9.0811, -12.4512,  ..., -31.1569, -31.4454,  -7.7317]]),\n",
       " tensor([[-15.3629,  -9.1023, -12.4839,  ..., -31.2461, -31.5609,  -7.7544]]),\n",
       " tensor([[-15.3649,  -9.1171, -12.5051,  ..., -31.3045, -31.6370,  -7.7747]]),\n",
       " tensor([[-15.3630,  -9.1351, -12.5268,  ..., -31.3655, -31.7164,  -7.7868]]),\n",
       " tensor([[-15.3577,  -9.1399, -12.5396,  ..., -31.4181, -31.7862,  -7.8043]]),\n",
       " tensor([[-15.3686,  -9.1587, -12.5721,  ..., -31.5143, -31.9103,  -7.8322]]),\n",
       " tensor([[-15.3599,  -9.1672, -12.5858,  ..., -31.5495, -31.9582,  -7.8410]]),\n",
       " tensor([[-15.3690,  -9.1836, -12.6187,  ..., -31.6601, -32.0968,  -7.8695]]),\n",
       " tensor([[-15.3652,  -9.1927, -12.6317,  ..., -31.6878, -32.1434,  -7.8782]]),\n",
       " tensor([[-15.3556,  -9.2036, -12.6447,  ..., -31.7300, -32.2042,  -7.8881]]),\n",
       " tensor([[-15.3660,  -9.2194, -12.6693,  ..., -31.8149, -32.3046,  -7.9139]]),\n",
       " tensor([[-15.3632,  -9.2194, -12.6812,  ..., -31.8725, -32.3832,  -7.9360]]),\n",
       " tensor([[-15.3562,  -9.2325, -12.6955,  ..., -31.9071, -32.4369,  -7.9417]]),\n",
       " tensor([[-15.3648,  -9.2461, -12.7177,  ..., -31.9708, -32.5209,  -7.9618]]),\n",
       " tensor([[-15.3590,  -9.2586, -12.7353,  ..., -32.0171, -32.5834,  -7.9699]]),\n",
       " tensor([[-15.3574,  -9.2576, -12.7484,  ..., -32.0832, -32.6663,  -7.9960]]),\n",
       " tensor([[-15.3549,  -9.2770, -12.7705,  ..., -32.1323, -32.7377,  -8.0013]]),\n",
       " tensor([[-15.3501,  -9.2786, -12.7757,  ..., -32.1644, -32.7874,  -8.0085]]),\n",
       " tensor([[-15.3407,  -9.2806, -12.7802,  ..., -32.1898, -32.8210,  -8.0192]]),\n",
       " tensor([[-15.3417,  -9.2899, -12.7969,  ..., -32.2427, -32.8969,  -8.0344]]),\n",
       " tensor([[-15.3472,  -9.3119, -12.8310,  ..., -32.3362, -33.0129,  -8.0541]]),\n",
       " tensor([[-15.3384,  -9.3077, -12.8245,  ..., -32.3439, -33.0329,  -8.0558]]),\n",
       " tensor([[-15.3398,  -9.3216, -12.8475,  ..., -32.3976, -33.1036,  -8.0699]]),\n",
       " tensor([[-15.3259,  -9.3165, -12.8398,  ..., -32.4055, -33.1241,  -8.0751]]),\n",
       " tensor([[-15.3285,  -9.3298, -12.8698,  ..., -32.4784, -33.2163,  -8.0894]]),\n",
       " tensor([[-15.3286,  -9.3392, -12.8776,  ..., -32.5087, -33.2593,  -8.0987]]),\n",
       " tensor([[-15.3226,  -9.3461, -12.8941,  ..., -32.5596, -33.3336,  -8.1019]]),\n",
       " tensor([[-15.3196,  -9.3500, -12.8998,  ..., -32.5897, -33.3748,  -8.1125]]),\n",
       " tensor([[-15.3160,  -9.3560, -12.9166,  ..., -32.6554, -33.4592,  -8.1292]]),\n",
       " tensor([[-15.3111,  -9.3614, -12.9251,  ..., -32.6784, -33.4945,  -8.1342]]),\n",
       " tensor([[-15.3031,  -9.3640, -12.9325,  ..., -32.7156, -33.5561,  -8.1369]]),\n",
       " tensor([[-15.3020,  -9.3742, -12.9412,  ..., -32.7316, -33.5776,  -8.1414]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_gpt2.predict('hello world', show_scores=True)['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "507bf2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1f39e589c640b7ba2025f0c2f10a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2938d1b652dd4fd2b2be143e7412cbd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 81912576\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training in autoregressive format...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:32, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./results/model_gpt2\n",
      "Configuration saved in ./results/model_gpt2/config.json\n",
      "Configuration saved in ./results/model_gpt2/generation_config.json\n",
      "Model weights saved in ./results/model_gpt2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "df_train_sample = df_train.head(10).copy()\n",
    "\n",
    "train_args = {\n",
    "    'title': 'gpt2',\n",
    "    'num_train_epochs' : 50,\n",
    "    'mlm': False,\n",
    "    'optimizer': 'adamw_torch',\n",
    "    'per_device_train_batch_size': 10,\n",
    "    'per_device_eval_batch_size': 10,\n",
    "    'warmup_steps': 20,\n",
    "    'weight_decay': 0.01,\n",
    "    'logging_steps': 10,\n",
    "    'output_dir': './results',\n",
    "    'logging_dir': './logs',\n",
    "    'save_model': True,\n",
    "}\n",
    "\n",
    "# Set training data and targets\n",
    "x = df_train_sample['question']\n",
    "y = df_train_sample['question']\n",
    "\n",
    "# Train model\n",
    "lm_gpt2_train.fit(x, y, train_args, instruct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7db089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'When did you know that the first time you saw a woman in a car, you were shocked to find that she was wearing a black dress.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_gpt2.predict('When did')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12274b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular? Beyonce was born? Beyonce was born in the U.S. and raised in the U.S. and was raised in the UK? Beyonce was a singer? Beyonce was a singer?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_gpt2_train.predict('When did', max_length=50)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150ddb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e0e66d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/spiece.model\n",
      "loading file tokenizer.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FLAN T5 base\n",
    "tokenizer_flan = AutoTokenizer.from_pretrained('google/flan-t5-base', mirror='https://huggingface.co')\n",
    "model_flan = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "model_flan_train=copy.deepcopy(model_flan)\n",
    "lm_flan = CreateHuggingFaceLM(model_flan, tokenizer_flan, input_block_size=10, padding_length=200)\n",
    "lm_flan_train = CreateHuggingFaceLM(model_flan_train, tokenizer_flan, input_block_size=10, padding_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "26e8a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8b345e161a4b878663ebdbb3d9dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654d9da3e69342d2a580d4709893543e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 247577856\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training in sequence to sequence format...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:15, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./results/model_flan_t5\n",
      "Configuration saved in ./results/model_flan_t5/config.json\n",
      "Configuration saved in ./results/model_flan_t5/generation_config.json\n",
      "Model weights saved in ./results/model_flan_t5/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "df_train_sample = df_train.head(10).copy()\n",
    "\n",
    "train_args = {\n",
    "    'title': 'flan_t5',\n",
    "    'num_train_epochs' : 50,\n",
    "    'mlm': False,\n",
    "    'optimizer': 'adamw_torch',\n",
    "    'per_device_train_batch_size': 10,\n",
    "    'per_device_eval_batch_size': 10,\n",
    "    'warmup_steps': 20,\n",
    "    'weight_decay': 0.01,\n",
    "    'logging_steps': 10,\n",
    "    'save_model': True,\n",
    "}\n",
    "\n",
    "# Set training data and targets\n",
    "x = df_train_sample['question']\n",
    "y = df_train_sample['answer']\n",
    "\n",
    "# Train model\n",
    "lm_flan_train.fit(x, y, train_args, instruct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7c3a4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'life is a cycle of life'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_flan.predict('What is the meaning of life?')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b05e57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Life is the meaning of all that comes to you.'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_flan_train.predict('What is the meaning of life?')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750129c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
