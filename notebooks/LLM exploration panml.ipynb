{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d888c13",
   "metadata": {},
   "source": [
    "## Assess various language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54128d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import openai\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "from transformers import TrainingArguments, Trainer, Seq2SeqTrainer, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfe1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device type\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "329f39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read OpenAI API key\n",
    "with open('openai_key.txt') as f:\n",
    "    lines = f.readlines()\n",
    "openai_key = lines[0]\n",
    "openai.api_key = openai_key # Set API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b8c792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_context='hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed3c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello, world! My name is John.\n"
     ]
    }
   ],
   "source": [
    "# LLM query - OpenAI\n",
    "response = openai.Completion.create(\n",
    "    model='text-davinci-002',\n",
    "    prompt=prompt_context,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    n=3,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")\n",
    "\n",
    "agent_response = response['choices'][0]['text']\n",
    "\n",
    "print(agent_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1e5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce9d831",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ad7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading question and answer data...\n",
      "Place data into dataframe object...\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "data_dir = os.path.join(base_dir, 'data', 'text_question_answer')\n",
    "\n",
    "print('Loading question and answer data...')\n",
    "with open (os.path.join(data_dir, 'train-squad-v2.0.json'), 'r') as f:\n",
    "    train_qna_raw = json.loads(f.read())\n",
    "\n",
    "\n",
    "print('Place data into dataframe object...')\n",
    "# Processing SQUAD data\n",
    "df_train = None\n",
    "ids = []\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "is_impossible = []\n",
    "for il1, l1 in enumerate(train_qna_raw['data']):\n",
    "    for il2, l2 in enumerate(l1['paragraphs']):\n",
    "        for il3, l3 in enumerate(l2['qas']):\n",
    "            ids.append(l3['id'])\n",
    "            contexts.append(l2['context'])\n",
    "            questions.append(l3['question'])\n",
    "            try:\n",
    "                answers.append(l3['answers'][0]['text'])\n",
    "            except:\n",
    "                answers.append('')\n",
    "            is_impossible.append(l3['is_impossible'])\n",
    "            \n",
    "df_train = pd.DataFrame({'id': ids, 'context': contexts, \n",
    "                         'question': questions, 'answer': answers, 'is_impossible': is_impossible})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af508e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>is_impossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "1  56be85543aeaaa14008c9065   \n",
       "2  56be85543aeaaa14008c9066   \n",
       "3  56bf6b0f3aeaaa14008c9601   \n",
       "4  56bf6b0f3aeaaa14008c9602   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question               answer  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "   is_impossible  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f989f3",
   "metadata": {},
   "source": [
    "### Text generation (casual language model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7f296f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace model\n",
    "class HuggingFaceModelPack():\n",
    "    '''\n",
    "    Generic model pack class for HuggingFace Hub models\n",
    "    '''\n",
    "    # Initialize class variables\n",
    "    def __init__(self, model, tokenizer, input_block_size, padding_length=100):\n",
    "        self.padding_length = padding_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.input_block_size = input_block_size\n",
    "        self.train_default_args = ['title', 'num_train_epochs', 'optimizer', 'mlm', \n",
    "                                   'per_device_train_batch_size', 'per_device_eval_batch_size',\n",
    "                                   'warmup_steps', 'weight_decay', 'logging_steps', \n",
    "                                   'output_dir', 'logging_dir', 'save_model']\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.model.config.eos_token_id\n",
    "    \n",
    "    # Embed text\n",
    "    def embedding(self, text):\n",
    "        token_ids = self.tokenizer.encode(text, return_tensors='pt')\n",
    "        \n",
    "        # Get embeddings\n",
    "        if 'flan' in self.model.name_or_path: \n",
    "            emb = self.model.shared.weight[token_ids[0]] # embeddings for FLAN\n",
    "        elif 'gpt2' in self.model.name_or_path: \n",
    "            emb = self.model.transformer.wte.weight[token_ids[0]] # embeddings for GPT2\n",
    "            \n",
    "        emb /= emb.norm(dim=1).unsqueeze(1) # normalise embedding weights\n",
    "        emb_pad = torch.zeros(self.padding_length, emb.shape[1]) # Set and apply padding to embeddings\n",
    "        emb_pad[:emb.shape[0], :] = emb\n",
    "        \n",
    "        return emb_pad\n",
    "    \n",
    "    # Generate text\n",
    "    def predict(self, text, max_length=100, skip_special_tokens=True, display_probability=False):\n",
    "        output_context = {\n",
    "            'text': None,\n",
    "            'probability': None,\n",
    "        }\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(text, return_tensors='pt')\n",
    "        output = self.model.generate(input_ids, \n",
    "                                     max_length=max_length,\n",
    "                                     pad_token_id=self.model.config.eos_token_id,\n",
    "                                     num_return_sequences=1, \n",
    "                                     output_scores=display_probability, \n",
    "                                     return_dict_in_generate=display_probability, \n",
    "                                     renormalize_logits=display_probability)\n",
    "                                          \n",
    "        \n",
    "\n",
    "        # Decode the output sequence\n",
    "        if display_probability:\n",
    "            output_context['text'] = self.tokenizer.decode(output['sequences'][0], skip_special_tokens=skip_special_tokens)\n",
    "            output_context['probability'] = [\n",
    "                {'token': self.tokenizer.decode(torch.argmax(nn.Softmax(dim=-1)(s)).item()), \n",
    "                 'probability': torch.max(nn.Softmax(dim=-1)(s)).item()} for s in output['scores']\n",
    "            ]\n",
    "        else:\n",
    "            output_context['text'] = self.tokenizer.decode(output[0], skip_special_tokens=skip_special_tokens)\n",
    "        output_context['text'] = output_context['text'].replace('\\n', '')\n",
    "        output_context['text'] = output_context['text'].strip()\n",
    "        \n",
    "        return output_context\n",
    "    \n",
    "    # Tokenize function\n",
    "    def _tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples['text'])\n",
    "    \n",
    "    # Tokenize pandas dataframe feature\n",
    "    def tokenize_text(self, x, batched=False):  \n",
    "        df_sample = pd.DataFrame({'text': x})\n",
    "        hf_dataset = Dataset.from_pandas(df_sample)\n",
    "        if batched:\n",
    "            tokenized_dataset = hf_dataset.map(self._tokenize_function, batched=batched, num_proc=4)\n",
    "        else:\n",
    "            tokenized_dataset = hf_dataset.map(self._tokenize_function)\n",
    "\n",
    "        return tokenized_dataset\n",
    "    \n",
    "    # Model training\n",
    "    def fit(self, x, y, train_args={}, instruct=False):\n",
    "        # Convert to tokens format from pandas dataframes\n",
    "        tokenized_data = self.tokenize_text(x)\n",
    "        tokenized_target = self.tokenize_text(y)\n",
    "        \n",
    "        # Check for missing input arguments\n",
    "        assert set(list(train_args.keys())) == set(self.train_default_args), \\\n",
    "        f'Train args are not in the required format - missing: {\", \".join(list(set(self.train_default_args) - set(list(train_args.keys()))))}'\n",
    "        \n",
    "        if instruct:\n",
    "            print('Setting up training in sequence to sequence format...')\n",
    "            tokenized_data = tokenized_data.add_column('labels', tokenized_target['input_ids']) # Create target sequence labels\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer=self.tokenizer, model=self.model) # Organise data for training\n",
    "            \n",
    "            # Setup training in sequence to sequence format\n",
    "            training_args = TrainingArguments(\n",
    "                optim=train_args['optimizer'], # model optimisation function\n",
    "                num_train_epochs=train_args['num_train_epochs'], # total number of training epochs\n",
    "                per_device_train_batch_size=train_args['per_device_train_batch_size'],  # batch size per device during training\n",
    "                per_device_eval_batch_size=train_args['per_device_eval_batch_size'],   # batch size for evaluation\n",
    "                warmup_steps=train_args['warmup_steps'], # number of warmup steps for learning rate scheduler\n",
    "                weight_decay=train_args['weight_decay'], # strength of weight decay\n",
    "                logging_steps=train_args['logging_steps'],\n",
    "                output_dir='./results', # output directory\n",
    "                logging_dir=train_args['logging_dir'], # log directory\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_data.remove_columns(['text']),\n",
    "                eval_dataset=tokenized_data.remove_columns(['text']),\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "    \n",
    "        else:\n",
    "            print('Setting up training in autoregressive format...')\n",
    "            data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=train_args['mlm']) # Organise data for training\n",
    "            \n",
    "            # Setup training in autoregressive format\n",
    "            training_args = TrainingArguments(\n",
    "                optim=train_args['optimizer'], \n",
    "                num_train_epochs=train_args['num_train_epochs'], \n",
    "                per_device_train_batch_size=train_args['per_device_train_batch_size'],\n",
    "                per_device_eval_batch_size=train_args['per_device_eval_batch_size'], \n",
    "                warmup_steps=train_args['warmup_steps'], \n",
    "                weight_decay=train_args['weight_decay'], \n",
    "                logging_steps=train_args['logging_steps'],\n",
    "                output_dir=train_args['output_dir'],\n",
    "                logging_dir=train_args['logging_dir'], \n",
    "            )\n",
    "            \n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_data.remove_columns(['text']),\n",
    "                eval_dataset=tokenized_data.remove_columns(['text']),\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "\n",
    "        trainer.train() # Execute training\n",
    "        \n",
    "        if train_args['save_model']:\n",
    "            trainer.save_model(f'./results/model_{train_args[\"title\"]}') # Save trained model\n",
    "    \n",
    "# OpenAI model\n",
    "class OpenAIModelPack():\n",
    "    '''\n",
    "    OpenAI model class\n",
    "    '''\n",
    "    def __init__(self, model, api_key):\n",
    "        self.model = model\n",
    "        openai.key = api_key\n",
    "    \n",
    "    # Generate text\n",
    "    def predict(self, text):\n",
    "        output_context = {\n",
    "            'text': None,\n",
    "            'probability': None,\n",
    "        }\n",
    "        response = openai.Completion.create(\n",
    "            model=self.model,\n",
    "            prompt=text,\n",
    "            temperature=0,\n",
    "            max_tokens=300,\n",
    "            top_p=1,\n",
    "            n=3,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "\n",
    "        output_context['text'] = response['choices'][0]['text']\n",
    "\n",
    "        return output_context\n",
    "    \n",
    "                     \n",
    "class ModelPack():\n",
    "    '''\n",
    "    Main model pack class\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer=None, \n",
    "                 input_block_size=10, \n",
    "                 padding_length=100, \n",
    "                 source='huggingface', \n",
    "                 api_key=None):\n",
    "        \n",
    "        self.padding_length = padding_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.input_block_size = input_block_size\n",
    "        self.source = source\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        # Accepted models from sources\n",
    "        self.accepted_models = {\n",
    "            'huggingface': [\n",
    "                'distilgpt2', \n",
    "                'google/flan-t5-base'\n",
    "            ],\n",
    "            'openai': [\n",
    "                'text-davinci-002', \n",
    "                'text-davinci-003'\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        # HuggingFace model call\n",
    "        if self.source == 'huggingface':\n",
    "            assert self.tokenizer is not None, 'tokenizer required for HuggingFace model'\n",
    "            self.instance = HuggingFaceModelPack(self.model, \n",
    "                                                 self.tokenizer, \n",
    "                                                 self.input_block_size, \n",
    "                                                 self.padding_length)\n",
    "        # OpenAI model call\n",
    "        elif self.source == 'openai':\n",
    "            assert self.api_key is not None, 'api key has not been specified'\n",
    "            assert self.model in self.accepted_models['openai'], 'model name is not found in accepted openai models: ' + ' '.join(self.accepted_models['openai'])\n",
    "            self.instance = OpenAIModelPack(model=self.model, api_key=self.api_key)\n",
    "                \n",
    "    # Direct to the attribute ofthe sub model pack class (attribute not found in the main model pack class)\n",
    "    def __getattr__(self, name):\n",
    "        return self.instance.__getattribute__(name)\n",
    "    \n",
    "    \n",
    "# Calculate cosine similarity of two matrices    \n",
    "def cosine_similarity(m1, m2):\n",
    "    return F.cosine_similarity(m1.view(1, -1), m2.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "205d8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_gpt3 = ModelPack(model='text-davinci-002', source='openai', api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ea87c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\n\\nHello, world! My name is John.', 'probability': None}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_gpt3.predict('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac382071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# GPT2 small\n",
    "tokenizer_dgpt2 = AutoTokenizer.from_pretrained('distilgpt2', mirror='https://huggingface.co')\n",
    "model_dgpt2 = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Create HuggingFace LM from base HuggingFace model and tokenizer objects\n",
    "model_dgpt2_train=copy.deepcopy(model_dgpt2)\n",
    "lm_gpt2 = ModelPack(model_dgpt2, tokenizer_dgpt2, input_block_size=10, padding_length=100, source='huggingface')\n",
    "lm_gpt2_train = ModelPack(model_dgpt2_train, tokenizer_dgpt2, input_block_size=10, padding_length=100, source='huggingface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b6f7eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world is a place where people can live and work together, and where people can live and work together, and where people can live and work together'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = lm_gpt2.predict('hello world is', max_length=30, display_probability=True)\n",
    "output['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17ccec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': ' a', 'probability': 0.05274742469191551},\n",
       " {'token': ' place', 'probability': 0.04598025605082512},\n",
       " {'token': ' where', 'probability': 0.4814596474170685},\n",
       " {'token': ' people', 'probability': 0.27657586336135864},\n",
       " {'token': ' can', 'probability': 0.2809840440750122},\n",
       " {'token': ' live', 'probability': 0.06638554483652115},\n",
       " {'token': ' and', 'probability': 0.26385536789894104},\n",
       " {'token': ' work', 'probability': 0.23955577611923218},\n",
       " {'token': ' together', 'probability': 0.2017972469329834},\n",
       " {'token': ',', 'probability': 0.24905261397361755},\n",
       " {'token': ' and', 'probability': 0.17192186415195465},\n",
       " {'token': ' where', 'probability': 0.06549273431301117},\n",
       " {'token': ' people', 'probability': 0.21751831471920013},\n",
       " {'token': ' can', 'probability': 0.6969819664955139},\n",
       " {'token': ' live', 'probability': 0.12064892798662186},\n",
       " {'token': ' and', 'probability': 0.7049790024757385},\n",
       " {'token': ' work', 'probability': 0.9612843990325928},\n",
       " {'token': ' together', 'probability': 0.9816696643829346},\n",
       " {'token': ',', 'probability': 0.45287710428237915},\n",
       " {'token': ' and', 'probability': 0.9863070845603943},\n",
       " {'token': ' where', 'probability': 0.9856353998184204},\n",
       " {'token': ' people', 'probability': 0.9493687748908997},\n",
       " {'token': ' can', 'probability': 0.9964258074760437},\n",
       " {'token': ' live', 'probability': 0.8176907896995544},\n",
       " {'token': ' and', 'probability': 0.9956592321395874},\n",
       " {'token': ' work', 'probability': 0.9978352189064026},\n",
       " {'token': ' together', 'probability': 0.9980382323265076}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698c885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "507bf2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e249e27dbdb446e5af6b5f65ea37cad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c108dafea8454a9e4bf35d052fa19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 81912576\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training in autoregressive format...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:30, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./results/model_gpt2\n",
      "Configuration saved in ./results/model_gpt2/config.json\n",
      "Configuration saved in ./results/model_gpt2/generation_config.json\n",
      "Model weights saved in ./results/model_gpt2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "df_train_sample = df_train.head(10).copy()\n",
    "\n",
    "train_args = {\n",
    "    'title': 'gpt2',\n",
    "    'num_train_epochs' : 50,\n",
    "    'mlm': False,\n",
    "    'optimizer': 'adamw_torch',\n",
    "    'per_device_train_batch_size': 10,\n",
    "    'per_device_eval_batch_size': 10,\n",
    "    'warmup_steps': 20,\n",
    "    'weight_decay': 0.01,\n",
    "    'logging_steps': 10,\n",
    "    'output_dir': './results',\n",
    "    'logging_dir': './logs',\n",
    "    'save_model': True,\n",
    "}\n",
    "\n",
    "# Set training data and targets\n",
    "x = df_train_sample['question']\n",
    "y = df_train_sample['question']\n",
    "\n",
    "# Train model\n",
    "lm_gpt2_train.fit(x, y, train_args, instruct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7db089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'When did you know that the first time you saw a woman in a car, you were shocked to find that she was wearing a black dress.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_gpt2.predict('When did')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12274b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular? Beyonce was born? Beyonce was born in the U.S. and raised in the U.S. and was raised in the UK? Beyonce was a singer? Beyonce was a singer?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_gpt2_train.predict('When did', max_length=50)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150ddb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e0e66d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/spiece.model\n",
      "loading file tokenizer.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/williamzheng/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FLAN T5 base\n",
    "tokenizer_flan = AutoTokenizer.from_pretrained('google/flan-t5-base', mirror='https://huggingface.co')\n",
    "model_flan = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "model_flan_train=copy.deepcopy(model_flan)\n",
    "lm_flan = ModelPack(model_flan, tokenizer_flan, input_block_size=10, padding_length=200, source='huggingface')\n",
    "lm_flan_train = ModelPack(model_flan_train, tokenizer_flan, input_block_size=10, padding_length=200, source='huggingface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "26e8a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8b345e161a4b878663ebdbb3d9dc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654d9da3e69342d2a580d4709893543e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n",
      "  Number of trainable parameters = 247577856\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training in sequence to sequence format...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:15, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./results/model_flan_t5\n",
      "Configuration saved in ./results/model_flan_t5/config.json\n",
      "Configuration saved in ./results/model_flan_t5/generation_config.json\n",
      "Model weights saved in ./results/model_flan_t5/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "df_train_sample = df_train.head(10).copy()\n",
    "\n",
    "train_args = {\n",
    "    'title': 'flan_t5',\n",
    "    'num_train_epochs' : 50,\n",
    "    'mlm': False,\n",
    "    'optimizer': 'adamw_torch',\n",
    "    'per_device_train_batch_size': 10,\n",
    "    'per_device_eval_batch_size': 10,\n",
    "    'warmup_steps': 20,\n",
    "    'weight_decay': 0.01,\n",
    "    'logging_steps': 10,\n",
    "    'save_model': True,\n",
    "}\n",
    "\n",
    "# Set training data and targets\n",
    "x = df_train_sample['question']\n",
    "y = df_train_sample['answer']\n",
    "\n",
    "# Train model\n",
    "lm_flan_train.fit(x, y, train_args, instruct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7c3a4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'life is a cycle of life'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_flan.predict('What is the meaning of life?')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b05e57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Life is the meaning of all that comes to you.'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_flan_train.predict('What is the meaning of life?')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750129c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
